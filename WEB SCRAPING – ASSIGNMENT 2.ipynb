{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "# 1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n",
    "# 2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "search_location = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_location.send_keys(\"Bangalore\")\n",
    "\n",
    "# 3. Then click the search button.\n",
    "\n",
    "search_button = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assistant/deputy Manager - Geo-spatial Data Analyst',\n",
       " 'Senior Data Analyst',\n",
       " 'Senior Data Analyst',\n",
       " 'Data Analyst / Engineer',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst (On Contract)',\n",
       " 'Senior Data Analyst',\n",
       " 'Senior Data Analyst for HANA Platform',\n",
       " 'Supervisor Role, Data Analyst, six sigma, Analytical skills, Database',\n",
       " 'Jr. Data Analyst/ Jr. Data Scientist']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "job_title = []\n",
    "job_location = []\n",
    "companies_name = []\n",
    "experience_required = []\n",
    "\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in titles:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gurgaon/Gurugram, bangalore',\n",
       " 'Kolkata, Hyderabad/Secunderabad, Pune, Ahmedabad, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)\\n(WFH during Covid)',\n",
       " 'Bengaluru/Bangalore',\n",
       " 'Chennai, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Hyderabad/Secunderabad, Bangalore/Bengaluru, Mumbai (All Areas)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for j in locations:\n",
    "    location=j.text\n",
    "    job_location.append(location)\n",
    "job_location[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maruti Suzuki India',\n",
       " 'SYREN TECHNOLOGIES PRIVATE LIMITED',\n",
       " 'Flipkart',\n",
       " 'Animaker India Private Limited',\n",
       " 'GlaxoSmithKline Pharmaceuticals Limited',\n",
       " 'Rupeek Fintech Pvt Ltd',\n",
       " 'Flipkart',\n",
       " 'Intel',\n",
       " 'Concentrix Daksh Services',\n",
       " 'Sejal Consulting Hub']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for k in companies:\n",
    "    company=k.text\n",
    "    companies_name.append(company)\n",
    "companies_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3-5 Yrs',\n",
       " '5-10 Yrs',\n",
       " '4-5 Yrs',\n",
       " '4-9 Yrs',\n",
       " '2-5 Yrs',\n",
       " '0-2 Yrs',\n",
       " '3-7 Yrs',\n",
       " '5-10 Yrs',\n",
       " '3-6 Yrs',\n",
       " '0-5 Yrs']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for e in experience:\n",
    "    exp=e.text\n",
    "    experience_required.append(exp)\n",
    "experience_required[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "jobs=pd.DataFrame({'Jobs':job_title, 'Locations':job_location, 'Companies':companies_name, 'Experience':experience_required})[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jobs</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Companies</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assistant/deputy Manager - Geo-spatial Data An...</td>\n",
       "      <td>Gurgaon/Gurugram, bangalore</td>\n",
       "      <td>Maruti Suzuki India</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...</td>\n",
       "      <td>SYREN TECHNOLOGIES PRIVATE LIMITED</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bengaluru/Bangalore</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>4-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst / Engineer</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>Animaker India Private Limited</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst (On Contract)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Rupeek Fintech Pvt Ltd</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Analyst for HANA Platform</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Intel</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Supervisor Role, Data Analyst, six sigma, Anal...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Concentrix Daksh Services</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jr. Data Analyst/ Jr. Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, M...</td>\n",
       "      <td>Sejal Consulting Hub</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Jobs  \\\n",
       "0  Assistant/deputy Manager - Geo-spatial Data An...   \n",
       "1                                Senior Data Analyst   \n",
       "2                                Senior Data Analyst   \n",
       "3                            Data Analyst / Engineer   \n",
       "4                                       Data Analyst   \n",
       "5                         Data Analyst (On Contract)   \n",
       "6                                Senior Data Analyst   \n",
       "7              Senior Data Analyst for HANA Platform   \n",
       "8  Supervisor Role, Data Analyst, six sigma, Anal...   \n",
       "9               Jr. Data Analyst/ Jr. Data Scientist   \n",
       "\n",
       "                                           Locations  \\\n",
       "0                        Gurgaon/Gurugram, bangalore   \n",
       "1  Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...   \n",
       "2                                Bengaluru/Bangalore   \n",
       "3                       Chennai, Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9  Hyderabad/Secunderabad, Bangalore/Bengaluru, M...   \n",
       "\n",
       "                                 Companies Experience  \n",
       "0                      Maruti Suzuki India    3-5 Yrs  \n",
       "1       SYREN TECHNOLOGIES PRIVATE LIMITED   5-10 Yrs  \n",
       "2                                 Flipkart    4-5 Yrs  \n",
       "3           Animaker India Private Limited    4-9 Yrs  \n",
       "4  GlaxoSmithKline Pharmaceuticals Limited    2-5 Yrs  \n",
       "5                   Rupeek Fintech Pvt Ltd    0-2 Yrs  \n",
       "6                                 Flipkart    3-7 Yrs  \n",
       "7                                    Intel   5-10 Yrs  \n",
       "8                Concentrix Daksh Services    3-6 Yrs  \n",
       "9                     Sejal Consulting Hub    0-5 Yrs  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "# 1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n",
    "# 2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "search_location = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_location.send_keys(\"Bangalore\")\n",
    "\n",
    "# 3. Then click the search button.\n",
    "\n",
    "search_button = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lead/Senior Data Scientist (NLP)',\n",
       " 'Senior Engineer - AIML - Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Jr. Data Analyst/ Jr. Data Scientist',\n",
       " 'Senior Data Scientist',\n",
       " 'Cognitive/AI Senior Data Scientist',\n",
       " 'Senior Data Scientist',\n",
       " 'Lead Programmer - Data Scientist',\n",
       " 'Senior Data Scientist',\n",
       " 'Data Scientist - Machine Learning']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "job_title = []\n",
    "job_location = []\n",
    "companies_name = []\n",
    "\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in titles:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru\\n(WFH during Covid)',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Palani, Bangalore/Bengaluru',\n",
       " 'Hyderabad/Secunderabad, Bangalore/Bengaluru, Mumbai (All Areas)',\n",
       " 'Pune, Bangalore/Bengaluru, Delhi / NCR',\n",
       " 'Bengaluru/Bangalore',\n",
       " 'Kolkata, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)\\n(WFH during Covid)',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru\\n(WFH during Covid)']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for j in locations:\n",
    "    location=j.text\n",
    "    job_location.append(location)\n",
    "job_location[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Samya.AI A FRACTAL Entity',\n",
       " 'Unisys',\n",
       " 'Groww',\n",
       " 'Sejal Consulting Hub',\n",
       " 'Wipro',\n",
       " 'IBM India Pvt. Limited',\n",
       " 'Wipro',\n",
       " 'GlaxoSmithKline Pharmaceuticals Limited',\n",
       " 'Juniper Networks',\n",
       " 'GormalOne LLP']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for k in companies:\n",
    "    company=k.text\n",
    "    companies_name.append(company)\n",
    "companies_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_2=pd.DataFrame({'Title':job_title, 'Locations':job_location, 'Companies':companies_name})[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Companies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lead/Senior Data Scientist (NLP)</td>\n",
       "      <td>Bangalore/Bengaluru\\n(WFH during Covid)</td>\n",
       "      <td>Samya.AI A FRACTAL Entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Engineer - AIML - Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Unisys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Palani, Bangalore/Bengaluru</td>\n",
       "      <td>Groww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jr. Data Analyst/ Jr. Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, M...</td>\n",
       "      <td>Sejal Consulting Hub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Pune, Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cognitive/AI Senior Data Scientist</td>\n",
       "      <td>Bengaluru/Bangalore</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Chennai...</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lead Programmer - Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Juniper Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Machine Learning</td>\n",
       "      <td>Bangalore/Bengaluru\\n(WFH during Covid)</td>\n",
       "      <td>GormalOne LLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title  \\\n",
       "0         Lead/Senior Data Scientist (NLP)   \n",
       "1  Senior Engineer - AIML - Data Scientist   \n",
       "2                      Lead Data Scientist   \n",
       "3     Jr. Data Analyst/ Jr. Data Scientist   \n",
       "4                    Senior Data Scientist   \n",
       "5       Cognitive/AI Senior Data Scientist   \n",
       "6                    Senior Data Scientist   \n",
       "7         Lead Programmer - Data Scientist   \n",
       "8                    Senior Data Scientist   \n",
       "9        Data Scientist - Machine Learning   \n",
       "\n",
       "                                           Locations  \\\n",
       "0            Bangalore/Bengaluru\\n(WFH during Covid)   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                        Palani, Bangalore/Bengaluru   \n",
       "3  Hyderabad/Secunderabad, Bangalore/Bengaluru, M...   \n",
       "4             Pune, Bangalore/Bengaluru, Delhi / NCR   \n",
       "5                                Bengaluru/Bangalore   \n",
       "6  Kolkata, Hyderabad/Secunderabad, Pune, Chennai...   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9            Bangalore/Bengaluru\\n(WFH during Covid)   \n",
       "\n",
       "                                 Companies  \n",
       "0                Samya.AI A FRACTAL Entity  \n",
       "1                                   Unisys  \n",
       "2                                    Groww  \n",
       "3                     Sejal Consulting Hub  \n",
       "4                                    Wipro  \n",
       "5                   IBM India Pvt. Limited  \n",
       "6                                    Wipro  \n",
       "7  GlaxoSmithKline Pharmaceuticals Limited  \n",
       "8                         Juniper Networks  \n",
       "9                            GormalOne LLP  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "# 1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "# 2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "# 3. Then click the search button.\n",
    "\n",
    "search_button = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "\n",
    "filter_location = driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[3]/label/i\") \n",
    "filter_location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_salary = driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/i\") \n",
    "filter_salary.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Hiring For Data Scientist',\n",
       " 'Project Manager | Team Leader | Senior Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'NTT DATA_ Hiring For BIG DATA ,DATA Scientist, Devops',\n",
       " 'Hiring For Data Analyst / Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist/ Senior Data Scientist']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "job_title = []\n",
    "job_location = []\n",
    "companies_name = []\n",
    "experience_required = []\n",
    "\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in titles:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Noida', ' Nodia'],\n",
       " ['Hyderabad/Secunderabad',\n",
       "  ' Pune',\n",
       "  ' Chennai',\n",
       "  ' Bangalore/Bengaluru',\n",
       "  ' Delhi / NCR',\n",
       "  ' Mumbai (All Areas)'],\n",
       " ['Noida',\n",
       "  ' Kolkata',\n",
       "  ' Hyderabad/Secunderabad',\n",
       "  ' Ahmedabad',\n",
       "  ' Chennai',\n",
       "  ' Coimbatore',\n",
       "  ' Bangalore/Bengaluru',\n",
       "  ' Delhi / NCR',\n",
       "  ' Mumbai (All Areas)'],\n",
       " ['Noida', ' Nodia'],\n",
       " ['Hyderabad/Secunderabad',\n",
       "  ' Pune',\n",
       "  ' Chennai',\n",
       "  ' Bangalore/Bengaluru',\n",
       "  ' Delhi / NCR',\n",
       "  ' Mumbai (All Areas)'],\n",
       " ['Noida',\n",
       "  ' Kolkata',\n",
       "  ' Hyderabad/Secunderabad',\n",
       "  ' Ahmedabad',\n",
       "  ' Chennai',\n",
       "  ' Coimbatore',\n",
       "  ' Bangalore/Bengaluru',\n",
       "  ' Delhi / NCR',\n",
       "  ' Mumbai (All Areas)'],\n",
       " ['Noida', ' Nodia'],\n",
       " ['Hyderabad/Secunderabad',\n",
       "  ' Pune',\n",
       "  ' Chennai',\n",
       "  ' Bangalore/Bengaluru',\n",
       "  ' Delhi / NCR',\n",
       "  ' Mumbai (All Areas)'],\n",
       " ['Noida',\n",
       "  ' Kolkata',\n",
       "  ' Hyderabad/Secunderabad',\n",
       "  ' Ahmedabad',\n",
       "  ' Chennai',\n",
       "  ' Coimbatore',\n",
       "  ' Bangalore/Bengaluru',\n",
       "  ' Delhi / NCR',\n",
       "  ' Mumbai (All Areas)'],\n",
       " ['Noida', ' Nodia'],\n",
       " ['Hyderabad/Secunderabad',\n",
       "  ' Pune',\n",
       "  ' Chennai',\n",
       "  ' Bangalore/Bengaluru',\n",
       "  ' Delhi / NCR',\n",
       "  ' Mumbai (All Areas)'],\n",
       " ['Noida',\n",
       "  ' Kolkata',\n",
       "  ' Hyderabad/Secunderabad',\n",
       "  ' Ahmedabad',\n",
       "  ' Chennai',\n",
       "  ' Coimbatore',\n",
       "  ' Bangalore/Bengaluru',\n",
       "  ' Delhi / NCR',\n",
       "  ' Mumbai (All Areas)']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations=driver.find_elements_by_xpath(\"//span[@class='ellipsis fleft fs12 lh16 halfwdt']\")\n",
    "for j in locations:\n",
    "    location=j.text.split(\",\") \n",
    "    job_location.append(location)\n",
    "job_location[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LG Electronics India Pvt. Ltd.',\n",
       " 'Navikenz India Pvt Ltd',\n",
       " 'Tata Consultancy Services Ltd.',\n",
       " 'Tidyquant (OPC) Private Limited',\n",
       " 'Teleperformance',\n",
       " 'NTT Data Business Solutions Pvt Ltd',\n",
       " 'Careerera',\n",
       " 'Whizhack Technologies pvt ltd',\n",
       " 'One Mobikwik Systems Private Limited',\n",
       " 'Newgen Software Technologies']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for k in companies:\n",
    "    company=k.text\n",
    "    companies_name.append(company)\n",
    "companies_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-2 Yrs',\n",
       " '2-7 Yrs',\n",
       " '4-9 Yrs',\n",
       " '1-5 Yrs',\n",
       " '4-9 Yrs',\n",
       " '3-8 Yrs',\n",
       " '1-3 Yrs',\n",
       " '2-5 Yrs',\n",
       " '2-5 Yrs',\n",
       " '2-5 Yrs']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for e in experience:\n",
    "    exp=e.text\n",
    "    experience_required.append(exp)\n",
    "experience_required[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "JT=job_title[:10]\n",
    "JL=job_location[:10]\n",
    "CN=companies_name[:10]\n",
    "Exp=experience_required[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(JT),len(JL), len(CN), len(Exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jobs</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Companies</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Noida,  Nodia]</td>\n",
       "      <td>LG Electronics India Pvt. Ltd.</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Hyderabad/Secunderabad,  Pune,  Chennai,  Ban...</td>\n",
       "      <td>Navikenz India Pvt Ltd</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>[Noida,  Kolkata,  Hyderabad/Secunderabad,  Ah...</td>\n",
       "      <td>Tata Consultancy Services Ltd.</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Project Manager | Team Leader | Senior Data Sc...</td>\n",
       "      <td>[Noida,  Nodia]</td>\n",
       "      <td>Tidyquant (OPC) Private Limited</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Hyderabad/Secunderabad,  Pune,  Chennai,  Ban...</td>\n",
       "      <td>Teleperformance</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NTT DATA_ Hiring For BIG DATA ,DATA Scientist,...</td>\n",
       "      <td>[Noida,  Kolkata,  Hyderabad/Secunderabad,  Ah...</td>\n",
       "      <td>NTT Data Business Solutions Pvt Ltd</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hiring For Data Analyst / Data Scientist</td>\n",
       "      <td>[Noida,  Nodia]</td>\n",
       "      <td>Careerera</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Hyderabad/Secunderabad,  Pune,  Chennai,  Ban...</td>\n",
       "      <td>Whizhack Technologies pvt ltd</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Noida,  Kolkata,  Hyderabad/Secunderabad,  Ah...</td>\n",
       "      <td>One Mobikwik Systems Private Limited</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist/ Senior Data Scientist</td>\n",
       "      <td>[Noida,  Nodia]</td>\n",
       "      <td>Newgen Software Technologies</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Jobs  \\\n",
       "0                                     Data Scientist   \n",
       "1                                     Data Scientist   \n",
       "2                          Hiring For Data Scientist   \n",
       "3  Project Manager | Team Leader | Senior Data Sc...   \n",
       "4                                     Data Scientist   \n",
       "5  NTT DATA_ Hiring For BIG DATA ,DATA Scientist,...   \n",
       "6           Hiring For Data Analyst / Data Scientist   \n",
       "7                                     Data Scientist   \n",
       "8                                     Data Scientist   \n",
       "9              Data Scientist/ Senior Data Scientist   \n",
       "\n",
       "                                           Locations  \\\n",
       "0                                    [Noida,  Nodia]   \n",
       "1  [Hyderabad/Secunderabad,  Pune,  Chennai,  Ban...   \n",
       "2  [Noida,  Kolkata,  Hyderabad/Secunderabad,  Ah...   \n",
       "3                                    [Noida,  Nodia]   \n",
       "4  [Hyderabad/Secunderabad,  Pune,  Chennai,  Ban...   \n",
       "5  [Noida,  Kolkata,  Hyderabad/Secunderabad,  Ah...   \n",
       "6                                    [Noida,  Nodia]   \n",
       "7  [Hyderabad/Secunderabad,  Pune,  Chennai,  Ban...   \n",
       "8  [Noida,  Kolkata,  Hyderabad/Secunderabad,  Ah...   \n",
       "9                                    [Noida,  Nodia]   \n",
       "\n",
       "                              Companies Experience  \n",
       "0        LG Electronics India Pvt. Ltd.    0-2 Yrs  \n",
       "1                Navikenz India Pvt Ltd    2-7 Yrs  \n",
       "2        Tata Consultancy Services Ltd.    4-9 Yrs  \n",
       "3       Tidyquant (OPC) Private Limited    1-5 Yrs  \n",
       "4                       Teleperformance    4-9 Yrs  \n",
       "5   NTT Data Business Solutions Pvt Ltd    3-8 Yrs  \n",
       "6                             Careerera    1-3 Yrs  \n",
       "7         Whizhack Technologies pvt ltd    2-5 Yrs  \n",
       "8  One Mobikwik Systems Private Limited    2-5 Yrs  \n",
       "9          Newgen Software Technologies    2-5 Yrs  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Finally create a dataframe of the scraped data.\n",
    "jobs_3=pd.DataFrame({'Jobs':JT, 'Locations':JL, 'Companies':CN, 'Experience':Exp}) \n",
    "jobs_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "# 1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "\n",
    "url = 'https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "# 2.Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "\n",
    "try:\n",
    "    driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "search_item = driver.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "search_item.send_keys(\"sunglasses\")\n",
    "\n",
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "Off = []\n",
    "\n",
    "for a in range(3):\n",
    "    bnd=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    for i in bnd:\n",
    "        bnds=i.text\n",
    "        Brand.append(bnds)\n",
    "    Brand[0:100]\n",
    "    \n",
    "        \n",
    "    pd=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    for j in pd:\n",
    "        pds=j.text\n",
    "        Product_Description.append(pds)\n",
    "    Product_Description[0:100]\n",
    "    \n",
    "        \n",
    "    price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    for k in price:\n",
    "        ps=k.text\n",
    "        Price.append(ps)\n",
    "    Price[0:100]\n",
    "    \n",
    "    \n",
    "    dsc=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\")\n",
    "    for l in dsc:\n",
    "        off=l.text\n",
    "        Off.append(off)\n",
    "    Off[0:100]\n",
    "    \n",
    "        \n",
    "next_btn = driver.find_element_by_xpath(\"//a[@class='_1LKTO3']/span\")\n",
    "next_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120 120\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand), len(Product_Description), len(Price), len(Off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sunglasses = pd.DataFrame({'Brand':Brand, 'Product Description':Product_Description, 'Price':Price, 'Off':Off})[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Off</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (56)</td>\n",
       "      <td>₹188</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹248</td>\n",
       "      <td>90% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹187</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹200</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹189</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection, Riding Glasses, Mirrored Wayfar...</td>\n",
       "      <td>₹187</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (32)</td>\n",
       "      <td>₹187</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection, Mirrored Round Sunglasses (Free...</td>\n",
       "      <td>₹209</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection, Polarized Aviator Sunglasses (32)</td>\n",
       "      <td>₹164</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product Description Price  \\\n",
       "0             SRPM             UV Protection Wayfarer Sunglasses (56)  ₹188   \n",
       "1        Elligator                UV Protection Round Sunglasses (54)  ₹248   \n",
       "2           PIRASO              UV Protection Aviator Sunglasses (54)  ₹187   \n",
       "3           PIRASO              UV Protection Aviator Sunglasses (54)  ₹200   \n",
       "4   kingsunglasses                UV Protection Round Sunglasses (54)  ₹189   \n",
       "..             ...                                                ...   ...   \n",
       "95  kingsunglasses  UV Protection, Riding Glasses, Mirrored Wayfar...  ₹187   \n",
       "96          PIRASO             UV Protection Wayfarer Sunglasses (32)  ₹187   \n",
       "97  ROZZETTA CRAFT  UV Protection, Gradient Rectangular Sunglasses...  ₹399   \n",
       "98      PHENOMENAL  UV Protection, Mirrored Round Sunglasses (Free...  ₹209   \n",
       "99          PIRASO   UV Protection, Polarized Aviator Sunglasses (32)  ₹164   \n",
       "\n",
       "        Off  \n",
       "0   85% off  \n",
       "1   90% off  \n",
       "2   88% off  \n",
       "3   87% off  \n",
       "4   82% off  \n",
       "..      ...  \n",
       "95  84% off  \n",
       "96  88% off  \n",
       "97  80% off  \n",
       "98  83% off  \n",
       "99  89% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART .\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "\n",
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "Rating = []\n",
    "Review_summary = []\n",
    "Full_review = []\n",
    " \n",
    "for a in range(10):\n",
    "    rate=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for i in rate:\n",
    "        ratings=i.text\n",
    "        Rating.append(ratings)\n",
    "    Rating[0:100]\n",
    "    \n",
    "        \n",
    "    rs=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    for j in rs:\n",
    "        r_s=j.text\n",
    "        Review_summary.append(r_s)\n",
    "    Review_summary[0:100]\n",
    "    \n",
    "        \n",
    "    fr=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    for k in fr:\n",
    "        f_r=k.text\n",
    "        Full_review.append(f_r)\n",
    "    Full_review[0:100]   \n",
    "        \n",
    "next_btn = driver.find_element_by_xpath(\"//a[@class='_1LKTO3']/span\")\n",
    "next_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Rating), len(Review_summary), len(Full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review summary</th>\n",
       "      <th>Full review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>So far it’s been an AMAZING experience coming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>i11 is worthy to buy, too much happy with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating       Review summary  \\\n",
       "0       5            Brilliant   \n",
       "1       5       Simply awesome   \n",
       "2       5  Best in the market!   \n",
       "3       5     Perfect product!   \n",
       "4       5    Worth every penny   \n",
       "..    ...                  ...   \n",
       "95      5            Fabulous!   \n",
       "96      5        Great product   \n",
       "97      4          Good choice   \n",
       "98      5    Worth every penny   \n",
       "99      5   Highly recommended   \n",
       "\n",
       "                                          Full review  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  This is my first iOS phone. I am very happy wi...  \n",
       "96  Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "97  So far it’s been an AMAZING experience coming ...  \n",
       "98  i11 is worthy to buy, too much happy with the ...  \n",
       "99  What a camera .....just awesome ..you can feel...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "iphone_11 = pd.DataFrame({'Rating':Rating, 'Review summary':Review_summary, 'Full review':Full_review})[0:100]\n",
    "iphone_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "url = 'https://www.flipkart.com' \n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "try:\n",
    "    driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "search_item = driver.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "search_item.send_keys(\"sneakers\")\n",
    "\n",
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "Off = []\n",
    "\n",
    "for a in range(3):\n",
    "    bnd=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    for i in bnd:\n",
    "        bnds=i.text\n",
    "        Brand.append(bnds)\n",
    "       \n",
    "        \n",
    "    pd=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    for j in pd:\n",
    "        pds=j.text\n",
    "        Product_Description.append(pds)\n",
    "        \n",
    "        \n",
    "    price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    for k in price:\n",
    "        ps=k.text\n",
    "        Price.append(ps)\n",
    "         \n",
    "    \n",
    "    dsc=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\")\n",
    "    for l in dsc:\n",
    "        off=l.text\n",
    "        Off.append(off)\n",
    "        \n",
    "        \n",
    "next_btn = driver.find_element_by_xpath(\"//a[@class='_1LKTO3']/span\")\n",
    "next_btn.click()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bnd = Brand [:100] \n",
    "PD = Product_Description[:100]\n",
    "Pr = Price[:100]\n",
    "Ds = Off[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Off</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIRDE</td>\n",
       "      <td>Men Casual Shoes Sneakers For Men</td>\n",
       "      <td>₹339</td>\n",
       "      <td>32% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BIRDE</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹423</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Sports Running Shoes Sneakers For Men</td>\n",
       "      <td>₹680</td>\n",
       "      <td>31% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIRDE</td>\n",
       "      <td>Super Stylish &amp; Trendy Combo Pack of 02 Pairs ...</td>\n",
       "      <td>₹322</td>\n",
       "      <td>35% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>₹536</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>BIRDE</td>\n",
       "      <td>Fashion Outdoor Canvas Casual Light Weight Lac...</td>\n",
       "      <td>₹339</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SCATCHITE</td>\n",
       "      <td>Numenzo Black Sneaker For Men Sneakers For Men</td>\n",
       "      <td>₹398</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ganpati traders</td>\n",
       "      <td>Casual Shoes Sneakers For Men</td>\n",
       "      <td>₹173</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Numenzo</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>56% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>BIRDE</td>\n",
       "      <td>Casual Sneakers Black Shoes For Men Sneakers F...</td>\n",
       "      <td>₹423</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand                                Product Description Price  \\\n",
       "0             BIRDE                  Men Casual Shoes Sneakers For Men  ₹339   \n",
       "1             BIRDE                                   Sneakers For Men  ₹423   \n",
       "2          RapidBox              Sports Running Shoes Sneakers For Men  ₹680   \n",
       "3             BIRDE  Super Stylish & Trendy Combo Pack of 02 Pairs ...  ₹322   \n",
       "4            Chevit      Modern Trendy Sneakers Shoes Sneakers For Men  ₹536   \n",
       "..              ...                                                ...   ...   \n",
       "95            BIRDE  Fashion Outdoor Canvas Casual Light Weight Lac...  ₹339   \n",
       "96        SCATCHITE     Numenzo Black Sneaker For Men Sneakers For Men  ₹398   \n",
       "97  ganpati traders                      Casual Shoes Sneakers For Men  ₹173   \n",
       "98          Numenzo                                   Sneakers For Men  ₹499   \n",
       "99            BIRDE  Casual Sneakers Black Shoes For Men Sneakers F...  ₹423   \n",
       "\n",
       "        Off  \n",
       "0   32% off  \n",
       "1   57% off  \n",
       "2   31% off  \n",
       "3   35% off  \n",
       "4   66% off  \n",
       "..      ...  \n",
       "95  65% off  \n",
       "96  75% off  \n",
       "97  57% off  \n",
       "98  56% off  \n",
       "99  62% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sneakers = pd.DataFrame({'Brand':Bnd, 'Product Description':PD, 'Price':Pr, 'Off':Ds})[0:100]\n",
    "sneakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.7"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image.\n",
    "Note: Applying the filter and scraping the data, everything should be done through code only and there\n",
    "should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "url = 'https://www.myntra.com/shoes' \n",
    "driver.get(url)\n",
    "driver.maximize_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_cbox = driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div\")\n",
    "price_cbox.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_cbox = driver.find_element_by_xpath(\"//li[@class='colour-listItem']\")\n",
    "colour_cbox.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "\n",
    "for a in range(2):\n",
    "    bnd=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "    for i in bnd:\n",
    "        bnds=i.text\n",
    "        Brand.append(bnds)\n",
    "    Brand[0:100]\n",
    "    \n",
    "        \n",
    "    pd=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "    for j in pd:\n",
    "        pds=j.text\n",
    "        Product_Description.append(pds)\n",
    "    Product_Description[0:100]\n",
    "    \n",
    "        \n",
    "    price=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "    for k in price:\n",
    "        ps=k.text\n",
    "        Price.append(ps)\n",
    "    Price[0:100]\n",
    "    \n",
    "        \n",
    "next_btn = driver.find_element_by_xpath(\"//a[@rel='next']\")\n",
    "next_btn.click()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand), len(Product_Description), len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Zoom Span 4 Running Shoes</td>\n",
       "      <td>Rs. 7195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>Men HOVR Apex 2 Training</td>\n",
       "      <td>Rs. 13999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men FLIGHT LEGACY Sneakers</td>\n",
       "      <td>Rs. 7995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men Deviate Nitro Running Shoe</td>\n",
       "      <td>Rs. 11249Rs. 14999(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASICS</td>\n",
       "      <td>Men GEL-KAYANO 27 Shoes</td>\n",
       "      <td>Rs. 11999Rs. 14999(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Leather Slip-On Sneakers</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Pavers England</td>\n",
       "      <td>Men Printed Leather Slip-On Sneakers</td>\n",
       "      <td>Rs. 10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Men Leather Chelsea Boots</td>\n",
       "      <td>Rs. 10710Rs. 11900(Rs. 1190 OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Tommy Hilfiger</td>\n",
       "      <td>Women Hybrid Leather Sneakers</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Calvin Klein</td>\n",
       "      <td>Men Solid Suede Sneakers</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                   Product Description  \\\n",
       "0             Nike         Men Zoom Span 4 Running Shoes   \n",
       "1     UNDER ARMOUR              Men HOVR Apex 2 Training   \n",
       "2             Nike            Men FLIGHT LEGACY Sneakers   \n",
       "3             Puma        Men Deviate Nitro Running Shoe   \n",
       "4            ASICS               Men GEL-KAYANO 27 Shoes   \n",
       "..             ...                                   ...   \n",
       "95    Hush Puppies          Men Leather Slip-On Sneakers   \n",
       "96  Pavers England  Men Printed Leather Slip-On Sneakers   \n",
       "97         Saint G             Men Leather Chelsea Boots   \n",
       "98  Tommy Hilfiger         Women Hybrid Leather Sneakers   \n",
       "99    Calvin Klein              Men Solid Suede Sneakers   \n",
       "\n",
       "                               Price  \n",
       "0                           Rs. 7195  \n",
       "1                          Rs. 13999  \n",
       "2                           Rs. 7995  \n",
       "3        Rs. 11249Rs. 14999(25% OFF)  \n",
       "4        Rs. 11999Rs. 14999(20% OFF)  \n",
       "..                               ...  \n",
       "95                          Rs. 9999  \n",
       "96                         Rs. 10999  \n",
       "97  Rs. 10710Rs. 11900(Rs. 1190 OFF)  \n",
       "98                          Rs. 7999  \n",
       "99                          Rs. 7999  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Black_Shoes = pd.DataFrame({'Brand':Brand, 'Product Description':Product_Description, 'Price':Price})[0:100]\n",
    "Black_Shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.8"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "url = 'https://www.amazon.in/' \n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "search_item = driver.find_element_by_xpath(\"//input[@class='nav-input nav-progressive-attribute']\")\n",
    "search_item.send_keys(\"laptop\")\n",
    "\n",
    "search_button = driver.find_element_by_xpath(\"//div[@class='nav-search-submit nav-sprite']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_type = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/span/div/div/div[5]/ul[1]/li[10]/span/a/div/label/i\") \n",
    "CPU_type.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = []\n",
    "titles=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "for i in titles:\n",
    "    title=i.text\n",
    "    title_name.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ratings = []\n",
    "rate=driver.find_elements_by_xpath(\"//span[class='a-size-base a-nowrap']\")\n",
    "for j in rate:\n",
    "    Rate=j.text\n",
    "    Ratings.append(Rate)\n",
    "Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Price = []\n",
    "price=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for k in price:\n",
    "    Ps=k.text\n",
    "    Price.append(Ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 0 28\n"
     ]
    }
   ],
   "source": [
    "print(len(title_name), len(Ratings), len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=title_name[:10]\n",
    "Pr=Price[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tiltle</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HP 15s 11th Gen Intel Core i3 15.6 inches Busi...</td>\n",
       "      <td>44,867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP 15s 11th Gen Intel Core i3 15.6 inches(39.6...</td>\n",
       "      <td>43,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenovo IdeaPad Slim 3 2021 Intel Core i3 11th ...</td>\n",
       "      <td>43,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dell 14 (2021) Thin &amp; Light i3-1005G1 Laptop, ...</td>\n",
       "      <td>38,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP 15s 11th Gen Intel Core i3 15.6 inches Busi...</td>\n",
       "      <td>44,867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Renewed) Lenovo Latitude Laptop X260 Intel Co...</td>\n",
       "      <td>19,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP 15 10th Gen Intel Core i3 Thin and Light 15...</td>\n",
       "      <td>40,910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS VivoBook 14 (2021), Intel Core i3-1115G4 ...</td>\n",
       "      <td>39,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo IdeaPad Slim 3 10th Gen Intel Core i3 1...</td>\n",
       "      <td>37,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dell 15 (2021) Laptop i3-1115G4, 8GB, 256GB SS...</td>\n",
       "      <td>41,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tiltle   Price\n",
       "0  HP 15s 11th Gen Intel Core i3 15.6 inches Busi...  44,867\n",
       "1  HP 15s 11th Gen Intel Core i3 15.6 inches(39.6...  43,490\n",
       "2  Lenovo IdeaPad Slim 3 2021 Intel Core i3 11th ...  43,490\n",
       "3  Dell 14 (2021) Thin & Light i3-1005G1 Laptop, ...  38,990\n",
       "4  HP 15s 11th Gen Intel Core i3 15.6 inches Busi...  44,867\n",
       "5  (Renewed) Lenovo Latitude Laptop X260 Intel Co...  19,990\n",
       "6  HP 15 10th Gen Intel Core i3 Thin and Light 15...  40,910\n",
       "7  ASUS VivoBook 14 (2021), Intel Core i3-1115G4 ...  39,990\n",
       "8  Lenovo IdeaPad Slim 3 10th Gen Intel Core i3 1...  37,990\n",
       "9  Dell 15 (2021) Laptop i3-1115G4, 8GB, 256GB SS...  41,990"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "laptop=pd.DataFrame({'Tiltle':TN, 'Price':Pr})[0:10]\n",
    "laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.9"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "# 1. First get the webpage https://www.ambitionbox.com/ \n",
    "\n",
    "url = 'https://www.ambitionbox.com/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "# 2. Click on the Job option as shown in the image.\n",
    "\n",
    "jobs = driver.find_element_by_xpath(\"//a[@class='link jobs']\")\n",
    "jobs.click()\n",
    "\n",
    "# 3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter “Data Scientist” and click on search button.\n",
    "\n",
    "search_input = driver.find_element_by_xpath(\"//input[@class='input tt-input']\") \n",
    "search_input.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_button = driver.find_element_by_xpath(\"//button[@class='ab_btn search-btn round']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_tab = driver.find_element_by_xpath(\"//div[@title='Location']\") \n",
    "location_tab.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. You will reach to the following web page click on location and in place of “Search location” enter “Noida” and select location “Noida”.\n",
    "search_location = driver.find_element_by_id(\"location_Noida\")\n",
    "search_location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LG Electronics India Pvt. Ltd.',\n",
       " 'GENPACT India Private Limited',\n",
       " 'GENPACT India Private Limited',\n",
       " 'NTT Data Business Solutions Pvt Ltd',\n",
       " 'GENPACT India Private Limited',\n",
       " 'GI Group',\n",
       " 'GI Group',\n",
       " 'GI Group',\n",
       " 'Steria India Ltd',\n",
       " 'Zyoin']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name = []\n",
    "titles=driver.find_elements_by_xpath(\"//p[@class='company body-medium']\")\n",
    "for i in titles:\n",
    "    title=i.text\n",
    "    company_name.append(title)\n",
    "company_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3d ago',\n",
       " '10d ago',\n",
       " '10d ago',\n",
       " '11d ago',\n",
       " '12d ago',\n",
       " '4d ago',\n",
       " '4d ago',\n",
       " '8d ago',\n",
       " '1mon ago',\n",
       " '5d ago']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_days = []\n",
    "days=driver.find_elements_by_xpath(\"//span[@class='body-small-l']\")\n",
    "for j in days:\n",
    "    day=j.text\n",
    "    no_of_days.append(day)\n",
    "Days=no_of_days[0::2]\n",
    "Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.1', '4.1', '4.0', '4.0', '4.0', '4.0', '4.0', '4.0', '4.1', '4.1']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = []\n",
    "ratings=driver.find_elements_by_xpath(\"//a[@class='rating rating-4']\")\n",
    "for k in ratings:\n",
    "    rate=k.text\n",
    "    rating.append(rate)\n",
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(company_name), len(Days), len(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>No. of days ago when job was posted</th>\n",
       "      <th>Rating of the company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LG Electronics India Pvt. Ltd.</td>\n",
       "      <td>3d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>10d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>10d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NTT Data Business Solutions Pvt Ltd</td>\n",
       "      <td>11d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>12d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GI Group</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GI Group</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GI Group</td>\n",
       "      <td>8d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Steria India Ltd</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zyoin</td>\n",
       "      <td>5d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Company Name No. of days ago when job was posted  \\\n",
       "0       LG Electronics India Pvt. Ltd.                              3d ago   \n",
       "1        GENPACT India Private Limited                             10d ago   \n",
       "2        GENPACT India Private Limited                             10d ago   \n",
       "3  NTT Data Business Solutions Pvt Ltd                             11d ago   \n",
       "4        GENPACT India Private Limited                             12d ago   \n",
       "5                             GI Group                              4d ago   \n",
       "6                             GI Group                              4d ago   \n",
       "7                             GI Group                              8d ago   \n",
       "8                     Steria India Ltd                            1mon ago   \n",
       "9                                Zyoin                              5d ago   \n",
       "\n",
       "  Rating of the company  \n",
       "0                   4.1  \n",
       "1                   4.1  \n",
       "2                   4.0  \n",
       "3                   4.0  \n",
       "4                   4.0  \n",
       "5                   4.0  \n",
       "6                   4.0  \n",
       "7                   4.0  \n",
       "8                   4.1  \n",
       "9                   4.1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data frame\n",
    "DS_Jobs_in_Noida=pd.DataFrame({})\n",
    "DS_Jobs_in_Noida['Company Name']=company_name\n",
    "DS_Jobs_in_Noida['No. of days ago when job was posted']=Days\n",
    "DS_Jobs_in_Noida['Rating of the company']=rating \n",
    "DS_Jobs_in_Noida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”.\n",
    "You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/acer/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "# 1. First get the webpage https://www.ambitionbox.com/ \n",
    "\n",
    "url = 'https://www.ambitionbox.com/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Click on the salaries option as shown in the image.\n",
    "\n",
    "salaries = driver.find_element_by_xpath(\"//a[@class='link salaries']\")\n",
    "salaries.click()\n",
    "\n",
    "# 3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”.\n",
    "\n",
    "driver.find_element_by_id('jobProfileSearchbox').send_keys(\"Data Scientist\", Keys.ENTER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "\n",
    "company_name = []\n",
    "company=driver.find_elements_by_xpath(\"//div[@class='name']\")\n",
    "for i in company:\n",
    "    title=i.text.split('\\n')[0]\n",
    "    company_name.append(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_salary_record = []\n",
    "TSR=driver.find_elements_by_xpath(\"//div[@class='name']\")\n",
    "for j in TSR:\n",
    "    sr=j.text.split('\\n')[1]\n",
    "    total_salary_record.append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_salary = [] \n",
    "mS=driver.find_elements_by_xpath(\"//div[@class='value body-medium']\")\n",
    "for k in mS:\n",
    "    msr=k.text.split(',')[0][0:10]\n",
    "    minimum_salary.append(msr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_salary = []\n",
    "AS=driver.find_elements_by_xpath(\"//p[@class='averageCtc']\")\n",
    "for l in AS:\n",
    "    asr=l.text \n",
    "    average_salary.append(asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_salary = [] \n",
    "MS=driver.find_elements_by_xpath(\"//div[@class='value body-medium']\")\n",
    "for m in MS:\n",
    "    Msr=m.text.split(',')[0]\n",
    "    maximum_salary.append(Msr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_required = []\n",
    "Exp=driver.find_elements_by_xpath(\"//div[@class='salaries sbold-list-header']\")\n",
    "for e in Exp:\n",
    "    exp=e.text.split('\\n')[2]\n",
    "    experience_required.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Total Salary Record</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Maximum Salary</th>\n",
       "      <th>Experience required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ab Inbev</td>\n",
       "      <td>based on 20 salaries</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 19.0L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZS</td>\n",
       "      <td>based on 12 salaries</td>\n",
       "      <td>₹ 23.0L</td>\n",
       "      <td>₹ 15.3L</td>\n",
       "      <td>₹ 23.0L</td>\n",
       "      <td>2 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Optum</td>\n",
       "      <td>based on 23 salaries</td>\n",
       "      <td>₹ 9.8L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 9.8L</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>based on 66 salaries</td>\n",
       "      <td>₹ 19.5L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 19.5L</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnitedHealth</td>\n",
       "      <td>based on 47 salaries</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 13.5L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>based on 26 salaries</td>\n",
       "      <td>₹ 21.3L</td>\n",
       "      <td>₹ 13.5L</td>\n",
       "      <td>₹ 21.3L</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Verizon</td>\n",
       "      <td>based on 14 salaries</td>\n",
       "      <td>₹ 9.5L</td>\n",
       "      <td>₹ 12.7L</td>\n",
       "      <td>₹ 9.5L</td>\n",
       "      <td>4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ganit Business Solutions</td>\n",
       "      <td>based on 13 salaries</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "      <td>₹ 12.4L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "      <td>4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ericsson</td>\n",
       "      <td>based on 42 salaries</td>\n",
       "      <td>₹ 7.2L</td>\n",
       "      <td>₹ 11.7L</td>\n",
       "      <td>₹ 7.2L</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Deloitte</td>\n",
       "      <td>based on 49 salaries</td>\n",
       "      <td>₹ 20.5L</td>\n",
       "      <td>₹ 11.2L</td>\n",
       "      <td>₹ 20.5L</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Company Name   Total Salary Record Minimum Salary  \\\n",
       "0                  Ab Inbev  based on 20 salaries        ₹ 15.0L   \n",
       "1                        ZS  based on 12 salaries        ₹ 23.0L   \n",
       "2                     Optum  based on 23 salaries         ₹ 9.8L   \n",
       "3         Fractal Analytics  based on 66 salaries        ₹ 19.5L   \n",
       "4              UnitedHealth  based on 47 salaries        ₹ 11.0L   \n",
       "5           Tiger Analytics  based on 26 salaries        ₹ 21.3L   \n",
       "6                   Verizon  based on 14 salaries         ₹ 9.5L   \n",
       "7  Ganit Business Solutions  based on 13 salaries        ₹ 22.0L   \n",
       "8                  Ericsson  based on 42 salaries         ₹ 7.2L   \n",
       "9                  Deloitte  based on 49 salaries        ₹ 20.5L   \n",
       "\n",
       "  Average Salary Maximum Salary Experience required  \n",
       "0        ₹ 19.0L        ₹ 15.0L         3-4 yrs exp  \n",
       "1        ₹ 15.3L        ₹ 23.0L           2 yrs exp  \n",
       "2        ₹ 15.0L         ₹ 9.8L         3-4 yrs exp  \n",
       "3        ₹ 15.0L        ₹ 19.5L         2-4 yrs exp  \n",
       "4        ₹ 13.5L        ₹ 11.0L         2-4 yrs exp  \n",
       "5        ₹ 13.5L        ₹ 21.3L         3-4 yrs exp  \n",
       "6        ₹ 12.7L         ₹ 9.5L           4 yrs exp  \n",
       "7        ₹ 12.4L        ₹ 22.0L           4 yrs exp  \n",
       "8        ₹ 11.7L         ₹ 7.2L         3-4 yrs exp  \n",
       "9        ₹ 11.2L        ₹ 20.5L         2-4 yrs exp  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Store the data in a dataframe.\n",
    "ambitionbox_DS_Jobs=pd.DataFrame({})\n",
    "ambitionbox_DS_Jobs['Company Name']=company_name [:10]\n",
    "ambitionbox_DS_Jobs['Total Salary Record']=total_salary_record [:10]\n",
    "ambitionbox_DS_Jobs['Minimum Salary']=minimum_salary[:10]\n",
    "ambitionbox_DS_Jobs['Average Salary']=average_salary[:10]\n",
    "ambitionbox_DS_Jobs['Maximum Salary']=maximum_salary[:10]\n",
    "ambitionbox_DS_Jobs['Experience required']=experience_required[:10]\n",
    "ambitionbox_DS_Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
